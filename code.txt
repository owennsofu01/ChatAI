import os
import json
import requests
import time
import argparse
import logging
import urllib.robotparser
import textract
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# --- Configuration ---
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
REQUEST_DELAY = 1  # in seconds

# Configure logging to provide detailed output
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Helper Functions ---
def get_page_title(soup):
    """
    Extracts the title from the HTML <title> tag.
    Returns the title as a string or a default if not found.
    """
    title_tag = soup.find('title')
    if title_tag and title_tag.string:
        full_title = title_tag.string.strip()
        return full_title.split(' - ')[0]
    return "Untitled"

def is_valid_url(url, base_url):
    """
    Checks if a URL is a valid, crawlable link within the domain.
    """
    parsed_url = urlparse(url)
    return parsed_url.scheme in ['http', 'https'] and parsed_url.netloc == urlparse(base_url).netloc and not parsed_url.fragment

def get_links_to_crawl(soup, current_url, base_url):
    """
    Extracts all valid, unvisited links from a page.
    """
    links = set()
    for anchor in soup.find_all('a', href=True):
        href = anchor.get('href')
        full_url = urljoin(current_url, href)
        if is_valid_url(full_url, base_url):
            links.add(full_url)
    return links

# --- Scraping Functions ---
def scrape_local_document(file_path):
    """
    Scrapes content from a local HTML, PDF, or DOCX file using textract.
    """
    scraped_documents = []
    logging.info(f"Attempting to scrape local file: {file_path}")
    
    try:
        file_extension = os.path.splitext(file_path)[1].lower()

        if file_extension in ['.pdf', '.doc', '.docx']:
            # Use textract for common document formats
            raw_text = textract.process(file_path).decode('utf-8')
            page_title = os.path.basename(file_path)
        
        elif file_extension == '.html':
            # Use BeautifulSoup for HTML files
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            soup = BeautifulSoup(content, 'html.parser')
            main_content = soup.find('body') or soup
            raw_text = main_content.get_text(separator=' ', strip=True)
            page_title = get_page_title(soup)
            
        else:
            logging.error(f"Unsupported file type: {file_extension}")
            return scraped_documents

        doc_id = os.path.basename(file_path).split('.')[0]
        document = {
            "id": doc_id,
            "text": raw_text,
            "metadata": {
                "source": file_path,
                "title": page_title
            }
        }
        scraped_documents.append(document)
        logging.info(f"Successfully scraped content from local file: {file_path}")
    
    except FileNotFoundError:
        logging.error(f"Error: The file {file_path} was not found.")
    except Exception as e:
        logging.error(f"Error scraping local file {file_path}: {e}")
        
    return scraped_documents

def scrape_website_content_crawler(base_url):
    """
    Crawls and scrapes a website starting from a specified URL.
    """
    scraped_documents = []
    to_visit = [base_url]
    visited_urls = set()
    
    # Check robots.txt before crawling
    rp = urllib.robotparser.RobotFileParser()
    try:
        rp.set_url(urljoin(base_url, '/robots.txt'))
        rp.read()
        if not rp.can_fetch('*', base_url):
            logging.warning(f"Crawling is disallowed by {base_url}'s robots.txt. Skipping.")
            return []
    except Exception as e:
        logging.warning(f"Could not read robots.txt for {base_url}: {e}")

    logging.info(f"Starting crawl from: {base_url}")

    while to_visit:
        full_url = to_visit.pop(0)

        if full_url in visited_urls:
            continue

        logging.info(f"Scraping page: {full_url}")
        visited_urls.add(full_url)
        
        try:
            response = requests.get(full_url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            main_content = soup.find('main') or soup.find('article') or soup.find('div', {'id': 'content'})
            if main_content:
                page_text = main_content.get_text(separator=' ', strip=True)
            else:
                page_text = soup.get_text(separator=' ', strip=True)
                
            page_title = get_page_title(soup)
            
            doc_id = os.path.basename(urlparse(full_url).path.strip('/')) or "home"
            document = {
                "id": doc_id,
                "text": page_text,
                "metadata": {
                    "source": full_url,
                    "title": page_title
                }
            }
            scraped_documents.append(document)
            
            new_links = get_links_to_crawl(soup, full_url, base_url)
            for link in new_links:
                if link not in visited_urls and link not in to_visit:
                    to_visit.append(link)

        except requests.exceptions.HTTPError as e:
            logging.error(f"HTTP error for {full_url}: {e}")
        except requests.exceptions.ConnectionError as e:
            logging.error(f"Connection error for {full_url}: {e}")
        except requests.exceptions.Timeout as e:
            logging.error(f"Timeout error for {full_url}: {e}")
        except requests.exceptions.RequestException as e:
            logging.error(f"Error scraping {full_url}: {e}")
        
        time.sleep(REQUEST_DELAY)

    return scraped_documents

# --- Main Execution ---
def main():
    parser = argparse.ArgumentParser(description="Scrape content from a website or local document.")
    parser.add_argument("--url", help="The URL of the website to crawl.")
    parser.add_argument("--file", help="The path to a local document to scrape (HTML, PDF, DOC, or DOCX).")
    args = parser.parse_args()

    content_data = []
    output_file_name = "scraped_data.json"

    if args.url:
        parsed_target = urlparse(args.url)
        base_url_for_crawl = f"{parsed_target.scheme}://{parsed_target.netloc}"
        output_file_name = f"{parsed_target.netloc.replace('.', '_')}_data.json"
        content_data = scrape_website_content_crawler(base_url_for_crawl)
    elif args.file:
        output_file_name = f"{os.path.basename(args.file).split('.')[0]}_data.json"
        content_data = scrape_local_document(args.file)
    else:
        logging.error("You must provide either a --url or a --file argument.")
        return

    if content_data:
        try:
            with open(output_file_name, 'w', encoding='utf-8') as f:
                json.dump(content_data, f, indent=4)
            logging.info(f"Successfully scraped {len(content_data)} document(s) and saved to {output_file_name}.")
        except Exception as e:
            logging.error(f"Failed to write to file. Error: {e}")
    else:
        logging.warning("No content was scraped. Exiting.")

if __name__ == "__main__":
    main()