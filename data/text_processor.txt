import os
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# --- Configuration ---
# Define the base URL of the website to scrape
BASE_URL = "https://hushbposervices.com"
# Define the specific pages (paths) to scrape
PAGES_TO_SCRAPE = [
    "/",
    "/careers/",
    "/contacts/",
    "/about/"
]
OUTPUT_FILE = "website_data.json"

# --- Scraping Logic ---
def scrape_website_content():
    """
    Scrapes a list of web pages and extracts their text content.
    Returns a list of dictionaries, where each dictionary represents a document.
    """
    scraped_documents = []
    
    # Define a custom User-Agent to mimic a browser
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Referer': BASE_URL,
    }

    print(f"Scraping content from: {BASE_URL}")

    for page_path in PAGES_TO_SCRAPE:
        # Correctly join the base URL and the page path
        full_url = urljoin(BASE_URL, page_path)
        print(f"  -> Scraping page: {full_url}")
        
        try:
            # Pass the headers with the request
            response = requests.get(full_url, headers=headers, timeout=10)
            response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract relevant text. This is a more refined approach.
            # We look for common content-containing tags and join their text.
            content_tags = soup.find_all(['p', 'article', 'main', 'h1', 'h2', 'h3'])
            page_text = ' '.join([tag.get_text(strip=True) for tag in content_tags])
            
            if not page_text.strip():
                print(f"Warning: No main content found on {full_url}. Falling back to full text.")
                page_text = soup.get_text(separator=' ', strip=True)

            # Create a document dictionary
            doc_id = os.path.basename(page_path.strip('/')) or "home"
            document = {
                "id": doc_id,
                "text": page_text,
                "metadata": {"source": full_url}
            }
            scraped_documents.append(document)
            
        except requests.exceptions.RequestException as e:
            print(f"Error scraping {full_url}: {e}")
            
    return scraped_documents

# --- Main Execution ---
if __name__ == "__main__":
    content_data = scrape_website_content()
    
    if content_data:
        try:
            with open(OUTPUT_FILE, 'w') as f:
                json.dump(content_data, f, indent=4)
            print(f"Successfully scraped content and saved to {OUTPUT_FILE}.")
        except Exception as e:
            print(f"Failed to write to file. Error: {e}")
    else:
        print("No content was scraped. Exiting.")
